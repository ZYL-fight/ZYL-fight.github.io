---
layout:     post   				    # 使用的布局（不需要改）
title:      过滤式特征选取				# 标题 
subtitle:   Relief算法族 #副标题
date:       2020-12-25 				# 时间
author:     ZYL 						# 作者
header-img:  img/pcr/114031.png  #这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
---

> 本帖为Relief算法族的学习记录帖
> 肝得太猛了，下次一定不通宵

## 前言

大家好，我是xxx，今天为大家讲解的是11.2节的过滤式选择



首先介绍一下什么是过滤式选择。过滤式选择是一种先对数据集进行特征选择，然后在训练学习器的选择方法。也就是说这个特征选择过程和后续训练学习器的过程，并不相关。因此可以视为先对初始的特征进行了“过滤”，再用过滤后的特征训练模型。



今天主要介绍一种经典的算法——Relief算法。同时由Relief算法衍生出了ReliefF算法和RReliefF算法。这三个算法统称为Relief算法族。其中Relief算法仅用于处理二分类问题，ReliefF算法经过改进后能处理多分类问题，而RReliefF算法则适用于处理回归问题。



### Relief算法

Relief算法只可用于二分类，且不能处理不完整的数据。算法的思想是统计每个特征的相关度，以此度量特征的重要性。因为一个好的特征应该能帮助区分类别，所以统计量也相应的高。对于特征的选取标准通常有两种：

+ 给定一个阈值τ，若某一特征的统计值大于此阈值，那么选取此特征
+ 制定选取特征个数k，选取统计值最大的k个特征


对于给定的训练集${(x_1,y_1),(x_2,y_2),……,(x_m,y_m)},I_i=(x_i,y_i)$，假设特征个数为$a$，特征统计量的向量为$W$，对于每个实例$R_i$，分别找出同类中猜中最近邻$H$(nearest-hit)和在异类中的猜错最近邻$M$(nearest-miss)。那么可以得到特征向量统计量的计算公式：

$W[A]:=W[A]-diff(A,R_i,H)+diff(A,R_i,M)$

其中定义diff方法

+ 若属性A上取值为离散值
  $diff(A,I_1,I_2)= \begin{cases} 0& {value(A,I_1)=value(A,I_2)}\\ 1& {otherwise} \end{cases}$ 

+ 若属性A上取值为连续值
$diff(A,I_1,I_2)= \frac{|value(A,I_1)-value(A,I_2)|}{max(A)-min(A)}$

  这个地方实际上是对距离进行了标准化处理，保证距离取值在[0,1]内

可以想象如果说一个特征分类效果特别好，假设此特征下的取值为离散值。那么上述的$diff(A,R_i,H)$应当取0，$diff(A,R_i,M)$应当取1，那么进行累加W[A]会越来越大。反之，若分类效果极差，那么上述的$diff(A,R_i,H)$应当取0，$diff(A,R_i,M)$应当取0，W[A]取值将一直为0。这就是用统计量衡量特征重要性的原理。



大家可能会好奇，为什么这里给出的计算公式与书上有所差异？经过查阅大量资料和比对，我得知此处的距离计算公式可以是欧式距离也可以是曼哈顿距离。在Relief算法的发明者的原文中，作者定义此距离为欧氏距离。而后，在ReliefF算法和RReliefF算法的发明者的原文中则采用了曼哈顿距离。在Murphy & Aha, 1995 一文中分别采用欧氏距离与曼哈顿距离对相同的数据集进行测试，得出结论：两种距离计算结果相差很小。我们的教材采用了最原始的文章中的定义，因此是欧氏距离计算公式，而我的PPT上选择的是曼哈顿距离，所以有所不同，但是两种计算方法都是正确可行的。



下面给出算法的伪代码

<img src="https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/img/blog/2020-12-25-1.png?raw=true" style="zoom:50%;" />



### ReliefF算法

ReliefF算法能处理多分类问题，具有更强的稳定性，能对缺失值进行处理。但是算法的思想依旧是统计特征的重要性。不同的是这里特征统计量计算发生了变化。
假设现在存在属于多个类的样本，对于某个实例$R_i$，找出每个类中找k个最近邻，记猜中近邻为$H$(near-hit)，猜错近邻为$M$(near-miss)。那么有以下统计量计算公式

$W[A]:=W[A]-\sum_{j=1}^{k}diff(A,R_i,H)/k+\sum_{C\neq class(R_i)}[\frac{P(C)}{1-P(class(R_i))}\sum_{j=1}^{k}diff(A,R_i,M_j(C))/k]$

这里与书上略有不同，选择多个近邻是为了进一步增强算法面对噪声的稳定性。



下面给出算法的伪代码：

<img src="https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/img/blog/2020-12-25-2.png?raw=true"  style="zoom:60%;" />



同时上述算法还提供了基于概率论对于缺失值的处理方法。

+ 若仅仅缺失一个值(e.g. $I_1$):
  
  $diff(A,I_1,I_2)=1-P(value(A,I_2)|class(I_1))$

  实际上就是计算$I_1$的类中取值与$I_2$不同的概率，当特征A越能区分两个类，这个概率值也就越趋近于1
+ 若缺失两个值:

  $diff(A,I_1,I_2)=1-\sum_{V}^{values(A)}(P(V|class(I_1))*P(V|class(I_2)))$

  实际上是计算了两个实例取值不同的概率，当特征A越能区分两个类，这个概率值也就越趋近于1



### RReliefF算法

RReliefF吸取Relief算法的思想，用于处理回归问题，一般和回归树结合使用。由于是回归问题，所以不能再用寻找猜中近邻和猜错近邻的方法，于是提出一种类似的统计量计算公式：

$W[A]= P(diff.value~of~A|nearest~inst.~from~diff~class)-P(diff.value~of~A|nearest~inst.~from~same~ class)$

这其实是用了概率作为相对距离，依旧是在不同类条件下两实例的A特征距离与相同类条件下两实例的A特征距离只差。核心思想不变。



由于需要处理连续值，所以需要重新定义diff方法:

$diff(A,I_1,I_2)=\begin{cases} 0& {d\leq t_{eq}}\\ 1& {d\geq t_{diff}}\\ \frac{d-t_{eq}}{t_{diff}-t_{eq}}&  t_{eq}\lt d\leq t_{diff} \end{cases}$

这里$t_{diff},t_{eq}$是用户自己定义的阈值参数，d为A特征下两实例的实际距离
接下来给出相应的概率公式：

$P_{diffA}=P(diff~value~of~A\|nearest~instances)=diff(A,R_1,I)$

$P_{diffC}=P(diff~prediction\|nearest~instances)=diff(\hat{C},R_1,I)$

$P_{diffC|diffA}=P(diff~prediction\|diff~value~of~A~and~nearest~instances)$

将$P_{diffA}$视作两实例在A特征上的相对距离。将$P_{diffC}$视为两实例预测值的相对距离，用于衡量不同类的概率。
对于$P(diff.value~of~A|nearest~inst.~from~diff~class)$可以有以下推导:

$P(diff.value~of~A\|nearest~inst.~from~diff~class)$

$=\frac{P(diff.value~of~A~from~\&~diff~class)}{P(diff~class)}$

$=\frac{P(from~diff~class\|diff.value~of~A)*P(diff.value~of~A)}{P(diff~class)}$

$=\frac{P_{diffC\|diffA}P_{diffA}}{P_{diffC}}$

同理可以推出$P(diff.value~of~A|nearest~inst.~from~same~ class)$
因此统计量计算公式变为:

$W[A]=\frac{P_{diffC\|diffA}P_{diffA}}{P_{diffC}}-\frac{(1-P_{diffC\|diffA})P_{diffA}}{1-P_{diffC}}$


下面给出伪代码:

<img src="https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/img/blog/2020-12-25-3.png?raw=true" style="zoom:60%;" />

这里伪代码的第6,8,10行在计算相对距离的时候，都乘上了一个系数。这是因为在回归问题中，通常情况需要考虑到：实例之间距离越近，影响越大。下面给出系数公式:

$d(i,j)=\frac{d_1(i,j)}{\sum_{l=1}^kd_1(i,j)}$

$d_1(i,j)=e^{-(\frac{rank(R_i,I_i)}{\sigma})^2}$

其中rank是升序排列的序号，σ是一个用户定义的一个参数，用来控制距离的影响程度



### 算法复杂度

对于n个实例，a个待选特征，m次迭代选择，Relief、ReliefF、RReliefF算法复杂度均为$O(m · n · a)$，因为算法复杂度主要体现在寻找近邻上，我们必须计算当前实例和所有其他实例的距离，当m被当做常数时，复杂度为$O(n·a)$（使用堆排序$O(k log n)$，使用kD树进一步降低复杂度)，这个复杂度体现了这个算法的高效性



### 总结

| 算法     | 优点                                                         | 缺点                                               |
| -------- | ------------------------------------------------------------ | -------------------------------------------------- |
| Relief   | 1. 效率高<br />2. 抗噪声且不受特征间关系影响                 | 1. 局限于二分类问题<br />2. 选择结果会出现冗余特征 |
| ReliefF  | 1. 处理多分类<br />2. 对缺失值可以处理<br />3. 更强的稳定性<br />4. 继承Relief优点 | 1. 选择结果会出现冗余特征                          |
| RReliefF | 1. 处理回归问题<br />2. 继承ReliefF优点                      | 1. 选择结果会出现冗余特征                          |



### 其他

这里列出了一些其他的一些没能提及的点：

1. 关于ReliefF算法族的一些抗噪声实验
2. 关于ReliefF和RReliefF近邻个数的选取，即k的取值
3. 关于连续值处理使用ramp函数
5. 关于样本大小和迭代次数的关系

稍微提一下第二点，这里当近邻影响力相同的时候，k一般取10，这个是ReliefF算法发明者通过实验给出的结论。至于其他的点，大家有兴趣的话可以查阅相关的文献，或者找我要一下资料。

下面是我这一部分的参考资料，那么我的部分就到此结束，感谢大家的聆听。


### 参考资料

[1] 周志华. 机器学习. 清华大学出版社, 2016: 249-250

<a href="https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/file/Robnik-Šikonja-Kononenko2003_Article_TheoreticalAndEmpiricalAnalysi.pdf">[2] Robnik-Šikonja M, Kononenko I. Theoretical and empirical analysis of ReliefF and RReliefF[J]. Machine learning, 2003, 53(1-2): 23-69.</a>

<a href="https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/file/AAAI92-020.pdf">[3] Kira K, Rendell L A. The feature selection problem: Traditional methods and a new algorithm[C]//Aaai. 1992, 2: 129-134.</a>

<a href="https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/file/kira1992.pdf">[4] Kira K, Rendell L A. A practical approach to feature selection[M]//Machine Learning Proceedings 1992. Morgan Kaufmann, 1992: 249-256.</a>

<a href="https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/file/Kononenko1994_Chapter_EstimatingAttributesAnalysisAn (1).pdf">[5]  Kononenko I. Estimating attributes: analysis and extensions of RELIEF[C]//European conference on machine learning. Springer, Berlin, Heidelberg, 1994: 171-182.</a>

<a href="https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/file/An_adaptation_of_Relief_for_attribute_estimation_i.pdf">[6] Robnik-Šikonja M, Kononenko I. An adaptation of Relief for attribute estimation in regression[C]//Machine Learning: Proceedings of the Fourteenth International Conference (ICML’97). 1997, 5: 296-304.</a>