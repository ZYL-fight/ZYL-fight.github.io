---
layout:     post   				    # 使用的布局（不需要改）
title:      图片爬取 				# 标题 
subtitle:                #副标题
date:       2020-11-15 				# 时间
author:     ZYL 						# 作者
header-img:  	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 爬虫
    - 多线程
---
## 图片爬取
>今天逛一个网站，发现里面有很多精美的图片，处于一个合格的色狼的本性，是想把她们全部下载下来，变成我的资源。但是一个一个点击下载实在是太繁琐了，于是打算写程序实现


先放个涩图以示敬意(nnk,yyds)![](https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/img/pcr/107031.png?raw=true)

大半年没碰爬虫了，今天重操旧业找点感觉，马上也要做相关的工作，权当是热热身。
#### 思路
找到的网站页面布局较为简单，可以简单的用lxml解析工具解析网页。可以得到图片的路径。在这个过程中，果然还是忘了点东西，在处理页面a标签的属性值时查了很久资料。最后用这个就简单地解决了，我不愧是个辣鸡。

```python
filename_temp = index.find('a')['href']
```
得到文件的链接属性后，直接可以知道下载路径，可是这个文件是webp类型的文件，并不方便我这个老色批欣赏，所以决定转换图片为png格式。查找资料后参考了<a>https://blog.csdn.net/wgPython/article/details/80740067</a>解决了问题。

#### 程序实现

##### picSpider.py
```python
from bs4 import BeautifulSoup
import requests
from downloadPic import download

headers= {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}
startUrl = '*********************'

print("【请求】网页数据")
response = requests.get(startUrl, headers=headers)
print("【成功】")
response.encoding = 'utf-8'
# print(response.text)
soup = BeautifulSoup(response.text, 'lxml')

for index in soup.find_all(class_='item'):
    # print(index)
    filename_temp = index.find('a')['href']
    # print("【获得】%s" %filename_temp)
    download(baseUrl=startUrl,subUrl=filename_temp)

```
##### downloadPic.py
```python
'''
参考 https://blog.csdn.net/wgPython/article/details/80740067
'''
import io
import time
from io import BytesIO
from PIL import Image  # 注意我的Image版本是pip3 install Pillow==4.3.0
import requests

save_path = "C:/Users/15190/Pictures/"

def download(subUrl,baseUrl):
    start_time = time.time()
    print("【开始下载】%s" %subUrl)
    res = requests.get(baseUrl+subUrl, stream=True)  # 获取字节流最好加stream这个参数,原因见requests官方文档
    print("【开始转换】%s" %subUrl)
    byte_stream = BytesIO(res.content)  # 把请求到的数据转换为Bytes字节流
    roiImg = Image.open(byte_stream)   # Image打开Byte字节流数据
    # roiImg.show()   #  弹出 显示图片
    imgByteArr = io.BytesIO()     # 创建一个空的Bytes对象
    roiImg.save(imgByteArr, format='PNG') # PNG就是图片格式，我试过换成JPG/jpg都不行

    imgByteArr = imgByteArr.getvalue()   # 这个就是保存的图片字节流

    filename = subUrl.replace("webp","png")
    with open(save_path + filename, "wb") as f:
        f.write(imgByteArr)
    print("【存储成功】%s 耗时%s" %(filename, time.time()-start_time))

```

#### 改进
初次测试后发现下载转换效率极低，大概40多秒才能存下来一张图片，一共150多张图片，那要下到猴年马月去啊。于是我很自然的想到了多线程处理，搞了一个线程池，让多个图片下载转换任务同时执行。处理后大概5分钟就可以下载完全部图片了，奈斯啊。
```python
import time
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup
import requests
from downloadPic import download
# 全局变量
headers= {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}
startUrl = '*********************'
temp_list = []

def multi_download():
    executor = ThreadPoolExecutor(max_workers=200)
    for temp in temp_list:
        executor.submit(download,baseUrl=startUrl,subUrl=temp)
    executor.shutdown()

if __name__ == '__main__':
    starttime = time.time()
    print("【请求】网页数据")
    response = requests.get(startUrl, headers=headers)
    print("【成功】")
    response.encoding = 'utf-8'
    # print(response.text)
    soup = BeautifulSoup(response.text, 'lxml')

    for index in soup.find_all(class_='item'):
        # print(index)
        filename_temp = index.find('a')['href']
        # print("【获得】%s" %filename_temp)
        temp_list.append(filename_temp)
    # 尝试多线程下载提高效率
    multi_download()
    print("【耗时】%s" %(time.time()-starttime))

```
（我是傻子，把编程的时间花在手动下载不早就下好了？？？）
![](https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/img/pcr/104931.png?raw=true)