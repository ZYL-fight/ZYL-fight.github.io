---
layout:     post   				    # 使用的布局（不需要改）
title:      大数据项目实践				# 标题 
subtitle:   南京二手房房子详情爬取 #副标题
date:       2020-11-27 				# 时间
author:     ZYL 						# 作者
header-img:  img/pcr/101231.png	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 爬虫
    - scrapy
---

> ​	连着十几天手头事情太多了，一直没写blog，今天刚好把这几天做的事情贴上来。
​	大数据的课程设计时间从原以为的两个月变更为两周……实在是太赶了，这几天一直在抓数据（老本行了），这一次没有其他人帮助，一个人完完整整的体会了整个流程，从设计到配置到编码到解决多方面问题，总体来说我对这次的完成情况相当满意，也更加深入理解了scrapy框架，感觉我已经无敌了（哈哈哈哈哈）。

![](https://github.com/ZYL-fight/ZYL-fight.github.io/blob/master/img/pcr/107131.png?raw=true)

# bigdata

#### 大数据项目简介

这个项目主要是做房价相关的大数据

#### 技术路线

##### 二手房

1. 初始url：https://nj.ke.com/ershoufang/
2. 找到区域里各个区的细节url，即herf属性，根据各个区的herf再得到下属街道的url。因为网站顶多展示3000左右的房子，想要获得全，必须分到各个街道。其中甚至出现了例如浦口区桥北区域内有10000+的房源，需要更加细的粒度，初步考虑根据房子的面积划分
   目标url形式应该是https://nj.ke.com/ershoufang/{地区拼音}/pg{页号}{a1-7}/ 
3. 请求目标url，解析HTML sellListContent类下clear类a标签中的herf属性值，即目标房子的详情网页界面
4. 网页中box-l类的div

#### 数据表设计

| 字段名           | 类型          | 意义                                   |
| ---------------- | ------------- | -------------------------------------- |
| name             | varchar(255)  | 房名                                   |
| district         | varchar(255)  | 所属区名                               |
| street           | varchar(255)  | 所属街道名                             |
| unit_build_time  | int(9)        | 房子建成时间                           |
| unit_type        | varchar(255)  | 户型                                   |
| area             | float(9,2)    | 建筑面积                               |
| inside_area      | float(9,2)    | 套内面积                               |
| build_type       | varchar(255)  | 建筑类型                               |
| build_struct     | varchar(255)  | 建筑结构                               |
| ele_house        | varchar(255)  | 梯户比                                 |
| layer            | varchar(255)  | 楼层                                   |
| unit_struct      | varchar(255)  | 户型结构                               |
| orientation      | varchar(255)  | 房屋朝向                               |
| renovation       | varchar(255)  | 装修情况                               |
| ele_exist        | varchar(255)  | 配备电梯                               |
| sold_time        | varchar(255)  | 挂牌时间                               |
| last_sold_time   | varchar(255)  | 上次挂牌时间                           |
| house_year       | varchar(255)  | 房屋年限                               |
| mortgage         | varchar(255)  | 抵押信息                               |
| trans_owner      | varchar(255)  | 交易权属                               |
| house_usage      | varchar(255)  | 房子用途                               |
| house_owner      | varchar(255)  | 财产所属                               |
| unit_detail      | varchar(2555) | 户型分间(因为这个没规律，考虑文本存储) |
| neigh_price      | float(9,2)    | 小区均价                               |
| neigh_build_time | int(9)        | 小区建成时间                           |
| unit_total_price | float(9,2)    | 房子总价                               |
| unit_per_price   | float(9,2)    | 房子平方价                             |
|                  |               |                                        |
|                  |               |                                        |

##### 新房

1. 初始url：https://nj.fang.ke.com/loupan/nhs1/
2. 直接进入二手房第三步，注意此处需要url拼接，先拿到街区
3. 数据并不多，进入后拿到房名，小区均价，总均价
4. 拿到clear house-det类div的data-id属性值1420039461800853，拼接url形如：https://nj.fang.ke.com/loupan/p_jagwlsabebt/huxingtu/1420039461800853.html
5. 拿到剩余数据

| 字段名            | 类型         | 意义       |
| ----------------- | ------------ | ---------- |
| name              | varchar(255) | 房名       |
| district          | varchar(255) | 所属区名   |
| street            | varchar(255) | 所属街道名 |
| unit_type         | varchar(255) | 户型       |
| area              | int(9)       | 面积       |
| unit_struct       | varchar(255) | 户型结构   |
| orientation       | varchar(255) | 房屋朝向   |
| unit_total_price  | int(9)       | 房子总价   |
| neigh_price       | int(9)       | 小区均价   |
| neigh_total_price | int(9)       | 小区总均价 |

#### 二手房项目运行方式

1. 配置setting,py中的相关数据库，mysql和redis，并且在mysql中新建数据库。数据表不需要新建，程序自动生成

2. setting中UA列表中的文件路径需要修改，ershouhouseurl_spider.py 文件中有使用文件的路径需要修改

3. 项目分为四个步骤

   + 首先获得所有街区级，按面积划分的url，以及每个url可翻的最大页数，程序在ershouhouseurl_spider.py 文件中 start_requests方法中被注释掉了，运行后得到的结果已经存放在totalUrl.txt文件中，大概需要等待20分钟完成（requests方式的ip代理没能实现，只能延时处理），所以此步骤可以不用执行

   + 运行命令

     ```powershell
     scrapy crawl ershou_house_url
     ```

     会在mysql中得到所有房子的详情url，此步骤大概需要15-20分钟，抓取近2000网页

   + 运行 mysql_conn_redis.py 文件，将所有的url导入redis

   + 运行命令

     ```powershell
     scrapy crawl ershou_house_detail
     ```

     可以开多个终端运行提高爬取效率，但是不能超过每秒请求IP代理的上限，目前是每秒不超过10个请求，结果会存入mysql
     
     此步骤由于实现了多开终端，目前来看同时开三个终端最稳定且效率高，每分钟大概获得400条左右数据，整个步骤完成大约5小时，爬取103000+网页

### 新房项目运行方式

由于新房暂时不予考虑，所以没有编写程序



### 满意之处

1. ip代理问题
2. 借助redis，多开终端
3. 爬虫中的异常处理

###### 项目码云地址：<a>https://gitee.com/zyl6/bigdata.git</a>